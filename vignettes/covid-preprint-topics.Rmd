---
title: "Example 1: Topic modelling Covid preprints"
output: rmarkdown::html_vignette
bibliography: srcquantcourse.bib
link-citations: yes
vignette: >
  %\VignetteIndexEntry{Example 1: Topic modelling Covid preprints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  cache = FALSE,
  comment = ">#"
)
```

```{r setup}
library(srcquantcourse)
library(dplyr)
library(tidyr)
library(stringr)
library(quanteda)
library(quanteda.textstats)
library(stm)
library(stminsights)
library(ggplot2)


```


```{css, echo=FALSE}
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```

# Introduction

This is an example topic modelling analysis using the *Structural Topic Model (STM)* [@RobertsStewart_et_2019_JSTATS_91] to explore a subset of [bioRxiv](https://www.biorxiv.org/) and [medRxiv](https://www.medrxiv.org/) preprints covering research related to *Covid-19*. The medRxiv website actually provides access to a dedicated collection of preprints on both medRxix and bioRxiv covering [COVID-19 SARS-CoV-2]((https://connect.medrxiv.org/relate/content/181)). Here we will however work with the whole preprint collection and create a subset using keyword matching. 

This document assumes some familiarity with *Structural Topic Modeling (STM)*. Please consult the `stm` package vignette [@RobertsStewart_et_2019_JSTATS_91] for background. 


# Getting the document data 

The [bioRxiv](https://www.biorxiv.org/) and [medRxiv](https://www.medrxiv.org/) servers [provide API access](https://api.biorxiv.org/) to retrieve preprint meta-data. One option to access the API is to use the  `[medrxivr](https://docs.ropensci.org/medrxivr/)` package. The code snippet below would for example collect all *bioRxiv* preprint meta-data for the year 2015. 

```{r eval=FALSE, echo=TRUE}
library(medrxivr)

biorxiv_raw <- mx_api_content(server = "biorxiv",
                              from_date = "2015-12-01",
                              to_date = "2015-12-31")
```

This package has been setup to support exploring different thematic preprint subsets and assumes usage of all preprints from both servers published from 2013 to 2023. This data can not be shared in this package as the [usage and API terms](https://www.medrxiv.org/tdm) of **bioRxiv/medRxiv do not permit redistribution and rehosting of the complete data**; therefore this package provides scripts to collect and clean the data in order to replicate this analysis (check `/data-raw` in the package repository).

The documented examples assume that you have obtained the data yourself and created a local copy for replication of the analyses. Below is the code that would allow you to do this. **NOTE** that at the time of writing this returned a dataset with *365526* records. Retrieving this data would take several hours. When replicating an analysis like this **use the bioRxiv/medRxiv API responsibly**. Consider collecting smaller datasets (the example shown below would for example only require preprints from 2020 to 2023). Alternatively, use the bulk snapshot as detailed [here](https://www.medrxiv.org/tdm).

```{r eval=FALSE, echo=TRUE}
library(dplyr)
library(medrxivr)

# get publications from medRxiv and bioRxiv
pubs_biorxiv_raw <- medrxivr::mx_api_content(server = "biorxiv",
                                             #from_date = "2019-01-01",
                                             to_date = "2023-12-31")

pubs_medrxiv_raw <- medrxivr::mx_api_content(server = "medrxiv",
                                             #from_date = "2019-01-01",
                                             to_date = "2023-12-31")

pubs_biorxiv_raw <- pubs_biorxiv_raw %>%
  mutate(server = "biorxiv")

pubs_medrxiv_raw <- pubs_medrxiv_raw %>%
  mutate(server = "medrxiv")

preprints_raw <- dplyr::bind_rows(pubs_biorxiv_raw, pubs_medrxiv_raw)

save(preprints_raw, file = "./data-raw/preprints_raw.Rdata")
```

In addition to the 15 meta-data variables returned via the API the code above adds `server` as a variable to indicate the origin of the preprint. This is an important variable for the examples shown below. 


# Cleaning, filtering and annotating the data

As a first step the raw preprint data has to be cleaned. Specifically, we want to retain only one unique record per preprint, multiple `version`s for each unique `doi` may exist. Below we retain only the latest version per doi and also ensure that there are no duplicate `doi`s. This is crucial as we need a unique identifier for each document that we want to include in the topic modelling. 


```{r eval=FALSE, echo=TRUE}
library(dplyr)

preprints_cleaned <- preprints_raw %>%
  group_by(doi) %>%
  filter(version == max(version)) %>%
  ungroup() %>%
  distinct(doi, .keep_all = TRUE)
```


Next we annotate the data with additional variables, specifically we want to reduce the publication `date` to the publication `year`, and add an additional variable `is_published`, which is inferred from the `published` variable. The latter provides the *DOI* of the journal where the preprint has been published after peer review.
We also limit the data to preprints from 2020 to 2023 (for an analysis of Covid topics preprints prior to 2020 are not relevant), and only retain variables that we may want to explore as *document covariates*, i.e. document propereties that potentially influence the prevalence of topics.


```{r eval=FALSE, echo=TRUE}
preprints <- preprints_cleaned %>%
  mutate(published = stringr::str_trim(published)) %>%
  mutate(published = na_if(published, "NA")) %>%
  mutate(is_published = as.numeric(!is.na(published))) %>%
  mutate(is_published = case_when(is_published == 1 ~ "published",
                                  is_published == 0 ~ "not published",
                                  TRUE ~ "undefined")) %>%
  mutate(year = lubridate::year(date)) %>%
  filter(year >= 2020 & year <= 2023) %>% 
  select(doi, server, title, abstract, date, year, version, is_published)
```


Finally, we define keywords and reduce our preprint set to preprints that contain one of those keywords in either the title or abstract, resulting in a subset of 29692 publications.

```{r eval=FALSE, echo=TRUE}
library(stringr)

keywords <- c("sars-cov", "covid")

search_pattern <- stringr::regex(paste(keywords, collapse = "|"), 
                                 ignore_case = TRUE)

covid_preprints <- preprints %>%
  filter(stringr::str_detect(title, pattern = search_pattern) |
           stringr::str_detect(abstract, pattern = search_pattern))
```


# Preparing and preprocessing the documents for text analysis

In order to analyze the preprints with the [`stm`](https://github.com/bstewart/stm) package we need to create a representation of the documents and document meta-data that `stm` can utilize. `stm` offers built-in methods to support this (see specifically the `textProcessor()` and `prepDocuments()` methods [@RobertsStewart_et_2019_JSTATS_91]). Here we use instead the [`quanteda`](http://quanteda.io/index.html) package [@BenoitWatanabe_et_2018_JOSS_3] which provides a broad range of methods for text pre-processing and analysis, creating formats also supported by `stm`.


## Create a corpus

First, we create a **corpus** object from the dataframe of preprints. The corpus is essentially a library of documents that will be used for the next steps. It specifies which variable should be used to uniquely identify documents and which variable holds the textual content (here the preprint *abstracts*) that should be processed.

Echoing the corpus will provide some basic information. All other variables in the original dataframe will be interpreted and included as document metadata (*'docvars'*), which could later be included in the STM topic modelling process. 

```{r eval=FALSE, echo=TRUE}
library(quanteda)

pubs_corpus <- covid_preprints %>%
  quanteda::corpus(docid_field = "doi", text_field = "abstract")

# pubs_corpus
# Corpus consisting of 29,692 documents and 6 docvars.
```


## Tokenize and preprocess

For further analysis the corpus documents have to be **tokenized**, i.e. further processing the texts have to be broken into semantic units that are relevant for our analysis. The most common approach is to interpret each *word* (typically designated by whitespaces or punctuation) as a token. This is applied here as well. `quanteda` offers several alternative approaches. Instead of individual words sequences of words (*n-grams*) could for example be used.

The tokenization method also provides several options for preprocessing and filtering the tokens. Here for example while tokenizing we will simultaneously remove punctuation, numbers, special symbols and URLs. Furthermore, we split words containing hyphens, a word like *social-ecological* will thus be split into two individual tokens (*social* and *ecological*). 

The text preprocessing choices could strongly influence the results of a text analysis [@DennySpirling_2018_PA_26] and should be thouroughly explained, carefully evaluated and ideally be based on theory.


```{r eval=FALSE, echo=TRUE}
pubs_tokens <- pubs_corpus %>%
  quanteda::tokens(remove_punct = TRUE,
                   remove_symbols = TRUE,
                   remove_numbers = TRUE,
                   remove_url = TRUE,
                   remove_separators = TRUE,
                   split_hyphens = TRUE) 

```


## Create a Document-feature matrix

The **tokens** object is then used to create a *document-feature matrix*. For further statistical analysis this reduces the tokens to a matrix of documents (rows) and unique **terms** (columns) that counts the number of occurrences for each term in each document. `quanteda` captures this as *features* which supports more general options than **terms** (see the `quanteda` documentation for details).


```{r eval=FALSE, echo=TRUE}
pubs_dfm <- pubs_tokens %>%
  quanteda::dfm()
```


## Filter terms and documents

This is followed by other (optional) processing and filtering steps. A common option for example  --- to reduce the size of the data or assist in the interpretation --- is the removal of so-called **stopwords** (e.g. *"the", "and", "or"* etc).

A step omitted here is reducing words (terms) to their word stem. The stemming algorithm (several are available) reduces words to its word stem. The terms "universal", "university" and "universe" would for example be reduced to the same word stem of "univers"; this example indicates that this approach may require careful consideration.

Stemming has the advantage that it could potentially reduce the size of the matrix substantially. 

```{r eval=FALSE, echo=TRUE}
pubs_dfm <- pubs_dfm %>%
  quanteda::dfm_remove(pattern = quanteda::stopwords("english")) #%>%
  #quanteda::dfm_wordstem()
```
```
# echo the result
> pubs_dfm

Document-feature matrix of: 29,692 documents, 82,472 features (99.87% sparse) and 6 docvars.
                features
docs             nitric oxide synthesised three isoforms synthases viz nnos neurons enos
  10.1101/038398      6     6           1     1        1         1   1    1       2    1
  10.1101/058511      0     0           0     0        0         0   0    0       0    0
  10.1101/292979      0     0           0     2        0         0   0    0       0    0
  10.1101/402370      0     0           0     0        0         0   0    0       0    0
  10.1101/420737      0     0           0     0        0         0   0    0       0    0
  10.1101/596700      0     0           0     0        0         0   0    0       0    0
[ reached max_ndoc ... 29,686 more documents, reached max_nfeat ... 82,462 more features ]
```

Further options may be considered to reduce noise and/or the size of the matrix. The following code removes for example terms (or features) that consist only of one character, terms that do not appear in at least two different documents, and furthermore would remove documents that do not contain at least 5 tokens. In our example this drops one document and reduces the number of retained features by more than half. 

```{r eval=FALSE, echo=TRUE}
pubs_dfm <- pubs_dfm %>%
  quanteda::dfm_remove(min_nchar = 2) %>%
  quanteda::dfm_trim(min_docfreq = 2, docfreq_type = "count") %>%
  quanteda::dfm_subset(quanteda::ntoken(.) > 4)
```
```
# echo the result
> pubs_dfm

Document-feature matrix of: 29,691 documents, 37,093 features (99.72% sparse) and 6 docvars.
                features
docs             nitric oxide synthesised three isoforms synthases viz neurons enos endothelial
  10.1101/038398      6     6           1     1        1         1   1       2    1           2
  10.1101/058511      0     0           0     0        0         0   0       0    0           0
  10.1101/292979      0     0           0     2        0         0   0       0    0           0
  10.1101/402370      0     0           0     0        0         0   0       0    0           0
  10.1101/420737      0     0           0     0        0         0   0       0    0           0
  10.1101/596700      0     0           0     0        0         0   0       0    0           0
[ reached max_ndoc ... 29,685 more documents, reached max_nfeat ... 37,083 more features ]
```

# Topic modeling

## Fitting the STM topic model

The key input is the number of topics (`K`) that the model should be fit to, which we here set to 20 (see the separate document for a discussion on suitable choices for the number of topics).

Before fitting the topic model we convert the document-feature matrix into the native STM format. In order to fit a topic model with the `stm()` function we need the set of `documents`, the `vocabulary` of which these documents are composed and a dataframe specifying the values of all document meta-data variables (`data`) which can be used in the process as "covariates" that might influence the prevalence of topics in a document. 

Here, in the example below, we ask `stm` to consider the origin of the document (`server`) and the publication `year` when fitting the topic model. The argument `prevalence = ~ server * s(year)` expresses that we assume that the prevalence of topics in a document is influenced by these two variables, and that they also interact, i.e. we work with the hypothesis that different temporal trends could be expected for documents published on either of the two preprint servers^[NOTE we use the recommended spline function for the `year` variable. Consult the STM documentation for details on this.].

The consideration of covariates is optional. If omitted the model reduces to a *Correlated Topic Model* [@BleiLafferty2007_AAS_1; @RobertsStewart_et_2019_JSTATS_91].

We also supply a `seed`, which allows to replicate the results of the topic modeling. 


```{r eval=FALSE, echo=TRUE}
library(stm)

covid_stm_docs <- quanteda::convert(pubs_dfm, to = "stm")

covid_model_K20 <- stm(documents = covid_stm_docs$documents,
                       vocab = covid_stm_docs$vocab,
                       data = covid_stm_docs$meta,
                       prevalence = ~ server * s(year),
                       K = 20,
                       verbose = TRUE,
                       seed = 9868467)
```


## Estimating the effect of document covariates

Once the model has converged we can estimate the effect of document covariates on the topic prevalence. The `estimateEffect()` function allows to run regressions based on the formula specified as the first argument. It is here identical to the formula used when fitting the topic model, and regressions are run for all 20 topics. The same metadata as used previously needs to be supplied for this function in addition to the topic model object.  

```{r eval=FALSE, echo=TRUE}
covid_effect_K20 <- estimateEffect(1:20 ~ server * s(year),
                                   stmobj = covid_model_K20,
                                   metadata = covid_stm_docs$meta)
```

This concludes fitting the model. The following sections step through a sample exploration of this topic model.


# Analysing and interpreting the topic model

## Basic topic model information

The topic model is defined by two matrices that capture probability distributions of topics over documents (*gamma* matrix) and words (or terms) over topics (*beta* matrix). We can start exploring these with some of the built-in functions of `stm`. 

The `plot()` function plots a chart showing topic proportions for all topics in the model. A topic is identified by a unique ID (1-20) and in the plot below the five words (or terms) that have the highest probability of being associated with the given topic. This gives an early indication of the distinct latent topics in the analysed subset of preprints. 

```{r echo=TRUE, dpi = 200, fig.height=7, fig.width=10}

plot(covid_model_K20, n = 5)

```

The `summary()` function provides a more detailed view of the topics and can help to begin interpreting and labeling the 20 topics. Specifically, the output shows four different sets of words associated with a topic. *'Highest Prob'* lists the words that have the highest probability of being associated with a topic. A comparison of different topics highlights that a term such as *covid* has a high probability for several topics. The list of *'FREX'* words summarizes words that are frequent and exclusive in a topic, i.e. characterize a topic in comparison to other topics (consult `stm::labelTopics()` for details as well as *Lift* and *Score* word sets). 

```{r echo=TRUE}

summary(covid_model_K20)

```

## Topic-document and term-topic distributions

As mentioned, the topic model is defined by the *gamma* (distribution of topics over words) and *beta* (distribution of terms over topics) matrices. With the help of the `tidytext` package we can extract those into dataframes for a more detailed analysis. Each row in the resulting dataframes lists the probability (`gamma`) of a given `topic` occurring in a given `document`^[NOTE that for each document a non-zero probability for all topics is assigned.].

```{r}
library(tidytext)

# retrieve the 'gamma' matrix
gamma <- tidytext::tidy(covid_model_K20, matrix = "gamma")

glimpse(gamma)
```

Similarly, the *beta* matrix (extracted into a dataframe) lists for each row the probability (`beta`) of a given `term` occurring in a given `topic`.

```{r}
library(tidytext)

# retrieve the 'beta' matrix
beta <- tidytext::tidy(covid_model_K20, matrix = "beta")

glimpse(beta)
```
Starting with the *beta* matrix we can create word clouds to explore useful semantic labels for each topic. 



## Understanding and labeling topics

As mentioned previously `stm` can use the word probabilities to also compute *FREX* (frequent and exclusive) words per topic. Those can be retrieved with the `stm::labelTopics()` function, which returns ordered lists of the different word sets characterizing a topic. We can combine the information about *high probability* and *FREX* words to create word clouds for each topic, which might help to assign a summary label for each topic.   

```{r echo=TRUE, warning=FALSE, fig.width=9, fig.height=11, fig.cap="**Word clouds showing the 50 most probable terms per topic.** Words are scaled by normalized probability per topic, terms that are also among the top 20 *FREX* terms are highlighted in orange. For readability, the terms 'sars', 'cov' and 'covid' have been removed as they occur with high probability in most topics."}
library(tidyr)
library(tibble)

# get the top FREX words
frex_top20 <- as.data.frame(labelTopics(covid_model_K20, n = 20)$frex) %>%
  rownames_to_column(var = "topic") %>%
  pivot_longer(starts_with("V"), values_to = "term") %>%
  mutate(is_frex = 1) %>%
  select(-name)

topic_words <- tidytext::tidy(covid_model_K20, matrix = "beta") %>%
  #filter(!(term %in% c("sars", "cov", "covid"))) %>% 
  mutate(topic = as.character(topic)) %>%
  group_by(topic) %>%
  arrange(-beta) %>%
  slice_head(n = 50) %>%
  mutate(beta_norm = (beta - min(beta)) / (max(beta) - min(beta))) %>%
  ungroup() %>%
  left_join(frex_top20, by = c("topic", "term")) %>%
  mutate(is_frex = ifelse(is.na(is_frex), "0", "1")) %>%
  filter(!(term %in% c("sars", "cov", "covid"))) %>%
  mutate(topic = paste("Topic", topic))

ggplot(topic_words, aes(label = term, size = beta_norm, color = is_frex)) +
  ggwordcloud::geom_text_wordcloud_area(shape = "square",
                                        rm_outside = TRUE) +
  scale_radius(range = c(2, 13)) +
  scale_color_manual(values = c("0" = "black", "1" = "#D55E00")) + 
  facet_wrap(~topic, ncol = 4)
```
From the review of this combination of *FREX* and *high probability* terms distinct topics are emerging, such as: "epidemic models" (Topic 1), "vaccines" (Topic 11), "testing" (Topic 16), "virus variants" (Topic 17), "treatments" (Topic 2), "mortality risks" (Topic 3), "mental health" (Topic 4), "country-wise case reports" (Topic 8), "virus molecular structure" (Topic 9).


## Covariate effects

A key feature of STM is the incorporation of document covariates into the topic model. In our example we considered the publication year and the preprint server as covariates that might influence the prevalence of a topic in a document. We also applied a regression for these covariates to the fit model, which were extracted with `stm::estimateEffect()`. 

As a first exploration we can print the regression tables for all or selected topics. 

```{r echo=TRUE}

summary(covid_effect_K20)

```

The following sections illustrate the exploration of covariate effects with several examples.


### Publication year

`stm` offers several built-in methods to explore the covariate effects visually. 

```{r echo=TRUE, fig.height=7, fig.width=7}
plot(covid_effect_K20,
     covariate = "year",
     method = "continuous",
     model = covid_model_K20,
     topics = c(1, 16, 11),
     xaxt = "n",
     main = 'Effect of publication year on prevalence of Topic 1 ("epidemic \nmodels"), Topic 16 ("testing") and Topic 11 ("vaccines")',
     labeltype = "prob",
     xlab = "Publication year")
axis(1, at = c("2020","2021","2022","2023"), labels = c(2020, 2021, 2022, 2023))
```
The visualization of the effect of preprint *publication year* on *expected topic proportions* confirm some trends that appear to match intuitively with different phases of the Covid-19 pandemic. Modelling the spread of infections (Topic 1) had a high relevance initially, but declined later, the same applies to (PCR-)testing (Topic 16), while vaccines (Topic 11) received limited coverage in earlier preprints, but became more important in later years.  

Alternatively, the `stminsights` package could be used to extract the same regression information from `stm` effects object and create customized charts.

```{r echo=TRUE, warning=FALSE, fig.width=9, fig.height=11}
library(stminsights)

year_effect <- get_effects(estimates = covid_effect_K20, 
                           variable = "year",
                           type = "continuous")

year_effect %>%
  mutate(topic = as.character(topic)) %>%
  mutate(topic = paste("Topic", topic)) %>%
    ggplot(aes(x = value, y = proportion)) +
      geom_line() +
      geom_ribbon(aes(ymin = lower, ymax = upper), 
                  alpha = 0.2, linetype = 0)  +
      xlab("Publication year") +
      ylab("Topic prevalence") +
      facet_wrap(~topic, ncol = 4) +
      theme_minimal()
```



### Preprint server 

The model also incorporated the preprint server (*bioRxiv* and *medRxiv*) as a covariate. We hypothesized that the prevalence of certain topics will be influenced by this covariate, i.e. that we are more likely to see certain topics on either *bioRxiv* and *medRxiv*. STM interpretes this as a *treatment* (see [@RobertsStewart_et_2014_AJPS_58] for background and examples) and we can extract the effect of this treatment from the regression object returned by `estimateEffect()`. The `stm::plot()` offers different methods to explore those effects. The figure below lists the effect of *'treatment medrxiv'* for all topics in the model. The treatment can have a positive or negative effect. Here we can for example see that Topic 4 ("mental health") can be expected with higher prevalence in *medRxiv* preprints, whereas Topic 9 ("virus molecular structure") will have a much lower prevalence in *medRxiv* preprints, i.e. should be expected with higher prevalence in *bioRxiv* preprints.


```{r echo=FALSE, fig.dpi=150, fig.height=14, fig.width=16}
plot(covid_effect_K20, 
     covariate = "server",
     #topics = c(9, 10, 16, 1),
     model = covid_model_K20, 
     method = "difference",
     cov.value1 = "medrxiv", cov.value2 = "biorxiv",
     xlab = "higher biorxiv prevalence ... higher medrxiv prevalence",
     xlim = c(-0.19, 0.1),
     #labeltype = "prob",
     main = "Effect of preprint server ('treatment medrxiv')")
```

We can also compare the topical prevalence effect of covariate values for a single topic. The figure below confirms the previous observation for Topic 9 ("virus molecular structure") which has a prevalence close to zero in *medRxiv* preprints but a prevalence of approximately 0.17 for *bioRxiv* preprints, i.e. it appears almost exclusively in the latter.


```{r echo=TRUE}
plot(covid_effect_K20, 
     covariate = "server",
     topics = c(9),
     model = covid_model_K20, 
      method = "pointestimate",
     xlab = "Topical prevalence",
     xlim = c(-0.04, 0.2),
     #labeltype = "prob",
     main = "Effect of preprint server covariate for Topic 9")

```


### Combination of preprint server and publication year

Finally, we can also explore the combined effect of covariates; we assumed an interacting effect of the two covariates `server` and `year`. The plot below visualizes this for Topic 17 ("virus variants") and illustrates that the topical prevalence increased throughout the pandemic in all preprints but that the topic apparently received more coverage in *bioRxiv* preprints.

```{r echo=FALSE, fig.width=10, fig.height=7, dpi=150}
plot(covid_effect_K20,
     topics = c(17),
     covariate = "year",
     model = covid_model_K20,
     ci.level = 0.95,
     method = "continuous",
     moderator = "server",
     moderator.value = "medrxiv",
     linecol = "#619CFF", lwd = 4,
     xlab = "Publication year",
     ylim = c(0, .15),
     main = 'Effect of preprint server on Topic 17 ("virus variants")',
     xaxt = "n",
     printlegend = F)
plot(covid_effect_K20,
     topics = c(17),
     covariate = "year",
     model = covid_model_K20,
     method = "continuous",
     moderator = "server",
     moderator.value = "biorxiv",
     linecol = "#F8766D", lwd = 2,
     add = T,
     printlegend = F)
legend(2020, .15, c("medRxiv", "bioRxiv"), lwd = 2, col = c("#619CFF", "#F8766D"))
axis(1, at = c("2020","2021","2022","2023"), labels = c(2020, 2021, 2022, 2023))
```

Again, `stminsights` can be used to retrieve this information and create customized visualizations for the combined covariate effect.


```{r echo=TRUE, warning=FALSE, fig.width=9, fig.height=11}
biorxiv_effect <- get_effects(covid_effect_K20,
                              variable = "year", type = "continuous", 
                              moderator = "server", modval = "biorxiv")

medrxiv_effect <- get_effects(covid_effect_K20,
                              variable = "year", type = "continuous", 
                              moderator = "server", modval = "medrxiv")

server_effects <- bind_rows(biorxiv_effect, medrxiv_effect)

server_effects %>%
  mutate(topic = as.character(topic)) %>%
  mutate(topic = paste("Topic", topic)) %>%
    ggplot(aes(x = value, y = proportion, color = moderator,
               group = moderator, fill = moderator)) +
      geom_line() +
      geom_ribbon(aes(ymin = lower, ymax = upper, 
                      fill = moderator), alpha = 0.2, linetype = 0) +
      xlab("Publication year") +
      ylab("Topic prevalence") +
      facet_wrap(~topic, ncol = 4) +
      theme_minimal() +
      theme(legend.position = "bottom") 
```





## Exploring the topic structure

### Topic correlations

Finally, considering that STM is extending [@RobertsStewart_et_2019_JSTATS_91] a correlated topic model [@BleiLafferty2007_AAS_1] we may also explore if topics are frequently cooccuring. A a topic correlation matrix is retrieved with `stm::topicCorr()` which can then be plotted. The resulting figure below indicates at least two clusters of topics.  

```{r echo=TRUE}
covid_topic_correlations <- topicCorr(covid_model_K20)

plot(covid_topic_correlations)
```

The correlation matrix can be used for further analysis (e.g. networks and clusters) or alternative visualizations. Both are outside the scope of this example and the course module.  For illustration, the following figure creates an alternative network visualization, with nodes coloured according to a community analysis of the topical network, which here reveals four distinct topical clusters.  

```{r eval=FALSE}
library(networkD3)

corrCutoff <- 0.01
topicModel <- covid_model_K20
nLabelWords <- 2

topic_correlation_matrix <- topicCorr(topicModel, cutoff = corrCutoff)

topic_igraph <- topic_correlation_matrix$poscor %>%
  igraph::graph.adjacency(mode = "undirected", weighted = TRUE, diag = FALSE)

topic_clusters <- igraph::cluster_louvain(topic_igraph)

d3_network <- topic_igraph %>%
  networkD3::igraph_to_networkD3(group = igraph::membership(topic_clusters))

topic_edges <- d3_network$links %>%
  mutate(value = value/min(value)) 

topic_labels <- as.data.frame(labelTopics(covid_model_K20, n = 4)$prob) %>%
  tibble::rownames_to_column(var = "topic_id") %>%
  tidyr::unite(col = "topic_label", starts_with("V"), sep = ", ")

node_centralities <- igraph::centr_betw(topic_igraph)$res
node_sizes <- data.frame(topic_id = seq_along(along.with = node_centralities),
                         size = node_centralities) %>%
  mutate(size = size/max(size))

# the name in the "nodes" DF is a factor, but the ordering has to be preserved
# when modifying the DF, otherwise the links are incorrectly interpreted
topic_nodes <- d3_network$nodes %>% 
  mutate(topic_id = as.numeric(row.names(.))) %>%
  merge(topic_labels, by = "topic_id") %>%
  merge(node_sizes, by = "topic_id") %>%
  rowwise() %>%
  mutate(name = paste(name, ": ", paste(topic_label, collapse = ", "), sep = "")) %>%
  arrange(topic_id)
  
topic_net <- networkD3::forceNetwork(Links = topic_edges, Nodes = topic_nodes,
                          Source = "source", Target = "target",
                          Value = "value", NodeID = "name",
                          Group = "group", Nodesize = "size",
                          radiusCalculation = JS("d.nodesize*20+4"),
                          #linkWidth = JS("function(d) { return d.value*5+1; }"),
                          charge = -250, 
                          #width = 1000, 
                          height = 500, 
                          opacity = 0.9, opacityNoHover = 1, 
                          fontFamily = "'Share Tech', 'Source Sans Pro'",
                          fontSize = 16, linkColour = "#c1c1c1",
                          colourScale = "d3.scaleOrdinal(d3.schemeCategory10)",
                          zoom = TRUE, bounded = TRUE)
topic_net
#saveNetwork(topic_net, "topic_correlation.html", selfcontained = TRUE)
```

```{r echo=FALSE}
htmltools::includeHTML("topic_correlation.html")
```


```{r eval=FALSE}
# potentially useful
#* topic signatures (table with primary, secondary etc topics) 
#* topic correlations (as heatmap) 
#* topic clusters (as network graph)
```





# References


